{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCs7P9JTMlzV"
   },
   "source": [
    "ASR implementation using pretrained wav2vec2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndG8MjmJeicp"
   },
   "source": [
    "# Fine-tuning Wav2Vec2 with an LM head\n",
    "\n",
    "In this notebook, we will load the pre-trained wav2vec2 model from [TFHub](https://tfhub.dev) and will fine-tune it on LJSpeech by appending Language Modeling head (LM) over the top of our pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWk8nL6Ui-_0"
   },
   "source": [
    "## Setting Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seqTlMyeZvM4",
    "outputId": "4f11ae64-73e7-49d4-f0c3-03a801479421"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3_fgx4eZvM7",
    "outputId": "b61d4835-7518-4304-ccf8-a53b3af3d1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from wav2vec2 import Wav2Vec2Config\n",
    "\n",
    "config = Wav2Vec2Config()\n",
    "\n",
    "print(\"TF version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0rVUxyWsS5f"
   },
   "source": [
    "Download the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NO6QRC7KZvM9"
   },
   "outputs": [],
   "source": [
    "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eiILuMBERxlO"
   },
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "\n",
    "AUDIO_MAXLEN = 246000\n",
    "\n",
    "LABEL_MAXLEN = 256\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V4gTgGLgXvO"
   },
   "source": [
    "In the following cell, we will wrap `pretrained_layer` & a dense layer (LM head) with the [Keras's Functional API](https://www.tensorflow.org/guide/keras/functional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a3CUN1KEB10Q"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
    "hidden_states = pretrained_layer(inputs)\n",
    "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zDXuoMXhDMo"
   },
   "source": [
    "The dense layer (defined above) is having an output dimension of `vocab_size` as we want to predict probabilities of each token in the vocabulary at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPp18ZHRtnq-"
   },
   "source": [
    "## Setting up training state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATQy1ZK3vFr7"
   },
   "source": [
    "In TensorFlow, model weights are built only when `model.call` or `model.build` is called for the first time, so the following cell will build the model weights for us. Further, we will be running `model.summary()` to check the total number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgL5wyaXZvM-",
    "outputId": "986d2ef0-0821-4c66-c017-67c2d6e470d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 246000)]          0         \n",
      "_________________________________________________________________\n",
      "keras_layer (KerasLayer)     (None, 768, 768)          94371712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768, 32)           24608     \n",
      "=================================================================\n",
      "Total params: 94,396,320\n",
      "Trainable params: 94,396,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQxxA4Fevp7m"
   },
   "source": [
    "Now, we need to define the `loss_fn` and optimizer to be able to train the model. The following cell will do that for us. We will be using the `Adam` optimizer for simplicity. `CTCLoss` is a common loss type that is used for tasks (like `ASR`) where input sub-parts can't be easily aligned with output sub-parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "glDepVEHZvM_"
   },
   "outputs": [],
   "source": [
    "from wav2vec2 import CTCLoss\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mvTuOXpwsQe"
   },
   "source": [
    "## Loading & Pre-processing data\n",
    "\n",
    "Let's now download the LJSpeech dataset from the [official website](https://keithito.com/LJ-Speech-Dataset/) and set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FeQfqVYXtuyK"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "5y3uq4citkMN",
    "outputId": "68a6afb7-4acc-4cdf-8423-5d6f686ac05c"
   },
   "outputs": [],
   "source": [
    "keras.utils.get_file(\n",
    "    os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
    "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
    "    extract=True,\n",
    "    archive_format=\"tar\",\n",
    "    cache_dir=\".\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AgMBmU_mtkRL"
   },
   "outputs": [],
   "source": [
    "saveto = \"./datasets/LJSpeech-1.1\"\n",
    "wavs = glob(f\"{saveto}/**/*.wav\", recursive=True)\n",
    "id_to_text = {line.strip().split(\"|\")[0]:line.strip().split(\"|\")[2] for line in open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42LZvWdkvdPO",
    "outputId": "aedf790c-6806-4c98-9ef2-90646debc5e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uhmYGxlDtkXh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.8.1-py3-none-any.whl (203 kB)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: decorator>=3.0.0 in d:\\asr\\venv\\lib\\site-packages (from librosa) (5.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\asr\\venv\\lib\\site-packages (from librosa) (21.2)\n",
      "Collecting audioread>=2.0.0\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scipy>=1.0.0\n",
      "  Downloading scipy-1.7.2-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "Collecting scikit-learn!=0.19.0,>=0.14.0\n",
      "  Using cached scikit_learn-1.0.1-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.5.2-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\asr\\venv\\lib\\site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in d:\\asr\\venv\\lib\\site-packages (from librosa) (0.10.3.post1)\n",
      "Collecting joblib>=0.14\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting numba>=0.43.0\n",
      "  Downloading numba-0.54.1-cp37-cp37m-win_amd64.whl (2.3 MB)\n",
      "Collecting llvmlite<0.38,>=0.37.0rc1\n",
      "  Using cached llvmlite-0.37.0-cp37-cp37m-win_amd64.whl (17.0 MB)\n",
      "Requirement already satisfied: setuptools in d:\\asr\\venv\\lib\\site-packages (from numba>=0.43.0->librosa) (58.4.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in d:\\asr\\venv\\lib\\site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: requests in d:\\asr\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (2.26.0)\n",
      "Collecting appdirs\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: six>=1.3 in d:\\asr\\venv\\lib\\site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\asr\\venv\\lib\\site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: pycparser in d:\\asr\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\asr\\venv\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\asr\\venv\\lib\\site-packages (from requests->pooch>=1.0->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\asr\\venv\\lib\\site-packages (from requests->pooch>=1.0->librosa) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\asr\\venv\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2.0.7)\n",
      "Building wheels for collected packages: audioread, resampy\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23154 sha256=6ae6d277a42e28cd25b06bc00b9ec9033cfdabac743ea2fd205d6a8c2198206f\n",
      "  Stored in directory: c:\\users\\harman\\appdata\\local\\pip\\cache\\wheels\\ba\\7b\\eb\\213741ccc0678f63e346ab8dff10495995ca3f426af87b8d88\n",
      "  Building wheel for resampy (setup.py): started\n",
      "  Building wheel for resampy (setup.py): finished with status 'done'\n",
      "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320731 sha256=7249212a5e8a0773b323a68bcf19c9ef053720132b7e006c190bba74cedac8fc\n",
      "  Stored in directory: c:\\users\\harman\\appdata\\local\\pip\\cache\\wheels\\a0\\18\\0a\\8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
      "Successfully built audioread resampy\n",
      "Installing collected packages: llvmlite, threadpoolctl, scipy, numba, joblib, appdirs, scikit-learn, resampy, pooch, audioread, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.9 joblib-1.1.0 librosa-0.8.1 llvmlite-0.37.0 numba-0.54.1 pooch-1.5.2 resampy-0.2.2 scikit-learn-1.0.1 scipy-1.7.2 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tNizK-nUtkaW"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def read_audio(file_path):\n",
    "    y,_=librosa.load(file_path,sr=16000)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lTajdERu0KvJ"
   },
   "outputs": [],
   "source": [
    "def get_data(wavs, id_to_text, maxlen=50):\n",
    "    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n",
    "    data = []\n",
    "    for w in wavs:\n",
    "        id = w.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        if len(id_to_text[id]) < maxlen:\n",
    "            data.append((read_audio(w),id_to_text[id]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zEoJDZa3tkdZ"
   },
   "outputs": [],
   "source": [
    "samples = get_data(wavs,id_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN2NLzCp0rRd",
    "outputId": "364897c9-a211-4236-8541-cd869e8733b1"
   },
   "outputs": [],
   "source": [
    "samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEObi_Apk3ZD"
   },
   "source": [
    "Alright, so each sub-directory has many `.flac` files and a `.txt` file. The `.txt` file contains text transcriptions for all the speech samples (i.e. `.flac` files) present in that sub-directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYW6WKJflO2e"
   },
   "source": [
    "We can load this text data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaat_hMLNVHF",
    "outputId": "18d30232-936a-469a-856c-9dd2fca7f7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading `vocab.json` from https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/vocab.json ... DONE\n"
     ]
    }
   ],
   "source": [
    "from wav2vec2 import Wav2Vec2Processor\n",
    "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
    "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    label = tokenizer(text)\n",
    "    return tf.constant(label, dtype=tf.int32)\n",
    "\n",
    "def preprocess_speech(audio):\n",
    "    audio = tf.constant(audio, dtype=tf.float32)\n",
    "    return processor(tf.transpose(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyKl8QP-zRFC"
   },
   "source": [
    "Now, we will define the python generator to call the preprocessing functions we defined in above cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PoQrRalwMpQ6"
   },
   "outputs": [],
   "source": [
    "def inputs_generator():\n",
    "    for speech, text in samples:\n",
    "        yield preprocess_speech(speech), preprocess_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Vlm3ySFULsG"
   },
   "source": [
    "## Setting up `tf.data.Dataset`\n",
    "\n",
    "Following cell will setup `tf.data.Dataset` object using its `.from_generator(...)` method. We will be using the `generator` object, we defined in the above cell.\n",
    "\n",
    "**Note:** For distributed training (especially on TPUs), `.from_generator(...)` doesn't work currently and it is recommended to train on data stored in `.tfrecord` format (Note: The TFRecords should ideally be stored inside a GCS Bucket in order for the TPUs to work to the fullest extent).\n",
    "\n",
    "You can refer to [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) for more details on how to convert LibriSpeech data into tfrecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LbQ_dMwGO62h"
   },
   "outputs": [],
   "source": [
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HXBbNsRyPyw3"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(wavs)\n",
    "SEED = 42\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DAUmns3pXfr"
   },
   "source": [
    "We will pass the dataset into multiple batches, so let's prepare batches in the following cell. Now, all the sequences in a batch should be padded to a constant length. We will use the`.padded_batch(...)` method for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Okhko1IWRida"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45CjQG5qSbV"
   },
   "source": [
    "Accelerators (like GPUs/TPUs) are very fast and often data-loading (& pre-processing) becomes the bottleneck during training as the data-loading part happens on CPUs. This can increase the training time significantly especially when there is a lot of online pre-processing involved or data is streamed online from GCS buckets. To handle those issues, `tf.data.Dataset` offers the `.prefetch(...)` method. This method helps in preparing the next few batches in parallel (on CPUs) while the model is making predictions (on GPUs/TPUs) on the current batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "f-bKu2YjRior"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqk2cs6LxVIh"
   },
   "source": [
    "Since this notebook is made for demonstration purposes, we will be taking first `num_train_batches` and will perform training over only that. You are encouraged to train on the whole dataset though. Similarly, we will evaluate only `num_val_batches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "z6GO5oYUxXtz"
   },
   "outputs": [],
   "source": [
    "split = 0.9\n",
    "\n",
    "num_train_batches = 5895\n",
    "num_val_batches = 655\n",
    "\n",
    "train_dataset = dataset.take(num_train_batches)\n",
    "val_dataset = dataset.skip(num_train_batches).take(num_val_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzAOI78tky08"
   },
   "source": [
    "## Model training\n",
    "\n",
    "For training our model, we will be directly calling `.fit(...)` method after compiling our model with `.compile(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vuBY2sZElgwg"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qswxafSl0HjO"
   },
   "source": [
    "The above cell will set up our training state. Now we can initiate training with the `.fit(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtuSfnj1l-I_",
    "outputId": "0a7a79cf-a89e-4aa1-bf42-1a0f164af82f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\ctc_ops.py:1442: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\ctc_ops.py:1442: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\ctc_ops.py:1425: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\ctc_ops.py:1425: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    199/Unknown - 336s 2s/step - loss: 281.4029"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_25928/4065518165.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mval_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1182\u001B[0m                 _r=1):\n\u001B[0;32m   1183\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1184\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1185\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1186\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    883\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    884\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 885\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    886\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    887\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    915\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    916\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 917\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    918\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    919\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3038\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m   3039\u001B[0m     return graph_function._call_flat(\n\u001B[1;32m-> 3040\u001B[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m   3041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3042\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1962\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1963\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1964\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1965\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1966\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    594\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    595\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 596\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    597\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    598\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mD:\\asr\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 60\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=1)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySvp8r2E1q_V"
   },
   "source": [
    "Let's save our model with `.save(...)` method to be able to perform inference later. You can also export this SavedModel to TFHub by following [TFHub documentation](https://www.tensorflow.org/hub/publish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0KEYcwydwjF"
   },
   "outputs": [],
   "source": [
    "save_dir = \"../Models/finetuned-wav2vec2\"\n",
    "model.save(save_dir, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkOpp9rZ211t"
   },
   "source": [
    "Note: We are setting `include_optimizer=False` as we want to use this model for inference only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJfPlTgezD0i"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now we will be computing Word Error Rate over the validation dataset\n",
    "\n",
    "**Word error rate** (WER) is a common metric for measuring the performance of an automatic speech recognition system. The WER is derived from the Levenshtein distance, working at the word level. Word error rate can then be computed as: WER = (S + D + I) / N = (S + D + I) / (S + D + C) where S is the number of substitutions, D is the number of deletions, I is the number of insertions, C is the number of correct words, N is the number of words in the reference (N=S+D+C). This value indicates the percentage of words that were incorrectly predicted. \n",
    "\n",
    "You can refer to [this paper](https://www.isca-speech.org/archive_v0/interspeech_2004/i04_2765.html) to learn more about WER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Io_91Y7-r3xu"
   },
   "source": [
    "We will use `load_metric(...)` function from [HuggingFace datasets](https://huggingface.co/docs/datasets/) library. Let's first install the `datasets` library using `pip` and then define the `metric` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GW9F_oVDU1TZ"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q datasets\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssWXWc7CZvNB"
   },
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def eval_fwd(batch):\n",
    "  logits = model(batch, training=False)\n",
    "  return tf.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFh1myg1x4ua"
   },
   "source": [
    "It's time to run the evaluation on validation data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQTFVjZghckJ"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for speech, labels in tqdm(val_dataset, total=num_val_batches):\n",
    "    predictions  = eval_fwd(speech)\n",
    "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
    "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
    "    metric.add_batch(references=references, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWCc8qBesv3e"
   },
   "source": [
    "We are using the `tokenizer.decode(...)` method for decoding our predictions and labels back into the text and will add them to the metric for `WER` computation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI_URj8Wtb2g"
   },
   "source": [
    "Now, let's calculate the metric value in following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a83wekLgWMod"
   },
   "outputs": [],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_cD1OgVEjl4"
   },
   "source": [
    "**Note:** Here metric value doesn't make any sense as the model is trained on very small data and ASR-like tasks often require a large amount of data to learn a mapping from speech to text. You should probably train on large data to get some good results. This notebook gives you a template to fine-tune a pre-trained speech model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G14o706kdTE1"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now that we are satisfied with the training process & have saved the model in `save_dir`, we will see how this model can be used for inference.\n",
    "\n",
    "First, we will load our model using `tf.keras.models.load_model(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrTrExiUdaED"
   },
   "outputs": [],
   "source": [
    "finetuned_model = tf.keras.models.load_model(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luodSroz20SR"
   },
   "source": [
    "Let's download some speech samples for performing inference. You can replace the following sample with your speech sample also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUE0shded6Ej"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/SA2.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycBjU_U53FjL"
   },
   "source": [
    "Now, we will read the speech sample using `soundfile.read(...)` and pad it to `AUDIO_MAXLEN` to satisfy the model signature. Then we will normalize that speech sample using the `Wav2Vec2Processor` instance & will feed it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z7CARje4d5_H"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11660/1678408418.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mspeech\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"SA2.wav\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mspeech\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspeech\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAUDIO_MAXLEN\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspeech\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mspeech\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocessor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconstant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspeech\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sf' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "speech, _ = sf.read(\"SA2.wav\")\n",
    "speech = np.pad(speech, (0, AUDIO_MAXLEN - len(speech)))\n",
    "speech = tf.expand_dims(processor(tf.constant(speech)), 0)\n",
    "\n",
    "outputs = finetuned_model(speech)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUSttSPa30qP"
   },
   "source": [
    "Let's decode numbers back into text sequence using the `Wav2Vec2tokenizer` instance, we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYdJqxQ4llgI"
   },
   "outputs": [],
   "source": [
    "predictions = tf.argmax(outputs, axis=-1)\n",
    "predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DXC757bztJc"
   },
   "source": [
    "This prediction is quite random as the model was never trained on large data in this notebook (as this notebook is not meant for doing complete training). You will get good predictions if you train this model on complete LibriSpeech dataset.\n",
    "\n",
    "Finally, we have reached an end to this notebook. But it's not an end of learning TensorFlow for speech-related tasks, this [repository](https://github.com/tulasiram58827/TTS_TFLite) contains some more amazing tutorials. In case you encountered any bug in this notebook, please create an issue [here](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBEm6caxYDyK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7Vlm3ySFULsG",
    "CzAOI78tky08",
    "G14o706kdTE1"
   ],
   "name": "asr_lc.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "184c0bf4d405d4a36e719b504ff2a22c838d19108535bf816dff1a5aad495b87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}